# Foundation LLM Testing Framework

A comprehensive framework for evaluating and comparing foundation language models for AI presentation generation.

## Overview

This framework provides a systematic approach to evaluate various large language models across multiple dimensions relevant to presentation generation, including reasoning capabilities, factual accuracy, instruction following, context utilization, and creative content generation.

## Supported Models

- **Anthropic**: Claude 3 Opus, Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3.7 Sonnet
- **OpenAI**: GPT-4o, GPT-4 Turbo, GPT-4.1, GPT-4.5 Preview
- **Google**: Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 2.0 Flash, Gemini 2.0 Flash Thinking, Gemini 2.5 Pro
- **Meta**: Llama 3 70B, Llama 3 8B, Llama 3.1, Llama 4 models
- **Mistral AI**: Mistral Small, Mistral Medium, Mistral Large
- **Others**: Databricks DBRX, Qwen 2.5 72B, Falcon 2 40B, Cohere Command R+, Cohere Command A, Cohere Command R

## Features

- **Unified Testing Pipeline**: Test all models with standardized test cases
- **Comprehensive Test Categories**: Reasoning, factual knowledge, instruction following, context handling, creative content generation
- **Presentation-Specific Tests**: PPT content generation, slide structure, meta-prompt creation
- **Automated Evaluation**: Quantitative and qualitative metrics for all test dimensions
- **Detailed Reporting**: YAML results, comparative matrices, visualizations
- **Cost Analysis**: Track and compare cost-efficiency metrics

## Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/foundation-llm-testing.git
   cd foundation-llm-testing